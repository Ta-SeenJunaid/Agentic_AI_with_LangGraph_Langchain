{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "801fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "\n",
    "arxiv = ArxivAPIWrapper(\n",
    "    top_k_results = 3,\n",
    "    ARXIV_MAX_QUERY_LENGTH = 300,\n",
    "    load_max_docs = 3,\n",
    "    load_all_available_meta = False,\n",
    "    doc_content_chars_max = 40000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c603e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-07-22\\nTitle: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\\nAuthors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\\nSummary: The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.\\n\\nPublished: 2021-07-16\\nTitle: All the attention you need: Global-local, spatial-channel attention for image retrieval\\nAuthors: Chull Hwan Song, Hye Joo Han, Yannis Avrithis\\nSummary: We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.\\n\\nPublished: 2023-06-02\\nTitle: RITA: Group Attention is All You Need for Timeseries Analytics\\nAuthors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\\nSummary: Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.run(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc3fd96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wiki = WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "008f32b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Artificial intelligence\\nSummary: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.run(\"What is AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21b5bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a:int, b:int) -> int:\n",
    "    \"\"\"Adds a and b\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a+b\n",
    "\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"Multiply a and b\n",
    "    \n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c88fdc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily = TavilySearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4b2e499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Provide me information about Brandenburg University of Technology',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://en.wikipedia.org/wiki/Brandenburg_University_of_Technology',\n",
       "   'title': 'Brandenburg University of Technology - Wikipedia',\n",
       "   'content': 'The **Brandenburg University of Technology Cottbus–Senftenberg** (German: _Brandenburgische Technische Universität_, **BTU**) was founded in 1991 and is a technical university in Brandenburg, Germany with campuses in Cottbus and Senftenberg. In February 2013 the Landtag of Brandenburg decided to merge the BTU and the Hochschule Lausitz on 1 July 2013 to create the new university Brandenburgische Technische Universität Cottbus-Senftenberg (abbreviated BTU). In two semesters, they get to know studying at the university and the variety of degree programmes at BTU Cottbus-Senftenberg. The BTU Cottbus-Senftenberg offers programmes for prospective students to acquire the university entrance qualification in the state of Brandenburg and language qualification. BTU Cottbus-Senftenberg offers nine degree programmes with double degrees or joint degrees with universities worldwide. _Brandenburg University of Technology_ (in German).',\n",
       "   'score': 0.8963332,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://yocket.com/universities/brandenburg-university-of-technology-2868',\n",
       "   'title': 'Brandenburg University of Technology: Acceptance Rate, Fees ...',\n",
       "   'content': 'The Brandenburg University of Technology is a technological university in Brandenburg, Germany. The institute was founded in 1991 as a building engineering',\n",
       "   'score': 0.8452414,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.youtube.com/watch?v=73Oj5vbb4Yk',\n",
       "   'title': 'Brandenburgische Technische Universität Cottbus–Senftenberg',\n",
       "   'content': 'Brandenburgische Technische Universität Cottbus–Senftenberg| Programs, Admission, Scholarships | Q&A\\nEducation Abroad Virtual Fair\\n941 subscribers\\n5 likes\\n444 views\\n27 Apr 2023\\nThe Brandenburg University of Technology Cottbus–Senftenberg (BTU) was founded in 1991 and is a technical university in Brandenburg, Germany with campuses in Cottbus and Senftenberg. The university has more than 7,000 students, of which 2,350 are of foreign origin from more than 100 nations.\\n\\nWe are a university of technology and develop practical application oriented solutions for the major global issues and transformation processes of the future with scientific competence. We are aware of our special responsibility when it comes to forward-looking and sustainable progress in our region.\\n\\nThe BTU offers study programmes in six different faculties. Ranging from Mathematics, Physics and Information Technology, Environment and Natural Sciences over Mechanical and Electrical Engineering, Architecture, Urban Planning and Civil Engineering up to Business, Social Sciences and Health & Life Sciences – BTU offers many different study programmes for you to grow.\\n\\nThe Brandenburg University of Technology Cottbus-Senftenberg is a campus university with three locations: Central Campus Cottbus, Campus Cottbus-Sachsendorf and Campus Senftenberg.\\n\\nAt the BTU, there are 180 professors (including visiting professors, junior professors and deputy professors) and 642 academic employees, in order to fully facilitate your studies and research.\\n\\nYou can watch all presentations and ask questions to a representative of the institution with just one click on the website: https://video.edu-abroad.com.ua/\\n\\nRegister now on the fair website to be the first to receive notifications about the start and schedule of the next event! https://online.edu-abroad.com.ua/\\n\\n\"Education Abroad\" - recordings of the presentations of current offers and opportunities for Ukrainian students to get an affordable education abroad! Each video consists of two parts: brief information about the educational institution, programs, dormitories, accommodation prices and assistance for students from Ukraine, etc., followed by a Q&A block, where representatives of higher education institutions answer urgent questions of visitors, providing more essential information.\\n\\nFollow us:\\nFacebook – https://www.facebook.com/EduAbroadVirtualFair\\nTelegram – https://t.me/edufaironline\\n\\n',\n",
       "   'score': 0.83093774,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.shiksha.com/studyabroad/germany/universities/brandenburg-university-of-technology',\n",
       "   'title': 'Brandenburg University of Technology (BTU): Ranking, Courses, Fee',\n",
       "   'content': 'Located in Cottbus, Germany, the Brandenburg University of Technology, is a public university, that first came into existence in the year 1991.',\n",
       "   'score': 0.7795405,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.kcrconsultants.com/brandenburg-university/',\n",
       "   'title': 'btu brandenburg university of technology cottbus-senftenberg',\n",
       "   'content': 'Image 4: BTU BRANDENBURG UNIVERSITY OF TECHNOLOGY COTTBUS-SENFTENBERG - KCR CONSULTANTS - Contact us Image 6: BTU BRANDENBURG UNIVERSITY OF TECHNOLOGY COTTBUS-SENFTENBERG - KCR CONSULTANTS - Contact us BTU Brandenburg University of Technology Cottbus-Senftenberg offers students the chance to engage in short-term or long-term international experiences, allowing them to enhance their skills and knowledge in an international and language setting. From environmental drives to educational outreach programs for the underprivileged, BTU Brandenburg University of Technology Cottbus-Senftenberg students are at the forefront, making a difference. *   BTU Brandenburg University of Technology Cottbus-Senftenberg is widely regarded as a prominent institution that embodies the principles of academic excellence, diversity and innovation.Established in 1991, this research-focused university boasts a rich knowledge dissemination and community engagement heritage.',\n",
       "   'score': 0.73914057,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.85,\n",
       " 'request_id': '8725fe1f-a8a8-4e63-810c-a1d7cf674de9'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"Provide me information about Brandenburg University of Technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe045b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73b5b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# Initialize utilities\n",
    "arxiv = ArxivAPIWrapper()\n",
    "wiki = WikipediaAPIWrapper()\n",
    "tavily = TavilySearch()\n",
    "\n",
    "# Wrap utilities into LangChain tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"arxiv_search\",\n",
    "        func=arxiv.run,\n",
    "        description=\"Searches Arxiv for academic papers\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"wiki_search\",\n",
    "        func=wiki.run,\n",
    "        description=\"Searches Wikipedia for general knowledge\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"tavily_search\",\n",
    "        func=tavily.run,\n",
    "        description=\"Searches the web using Tavily\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"add\",\n",
    "        func=lambda x, y: x + y,\n",
    "        description=\"Adds two numbers\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"multiply\",\n",
    "        func=lambda x, y: x * y,\n",
    "        description=\"Multiplies two numbers\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"subtract\",\n",
    "        func=lambda x, y: x - y,\n",
    "        description=\"Subtracts two numbers\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"divide\",\n",
    "        func=lambda x, y: x / y,\n",
    "        description=\"Divides two numbers\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm=ChatGroq(model=\"qwen/qwen3-32b\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171adca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
